[Data]
pretrained_embeddings_file = ../pretrain/cc.chinese.300.filter.txt
data_dir = ../processed/chinese-unified
train_file = %(data_dir)s/train_pro
raw_dev_file = %(data_dir)s/dev_raw
pro_dev_file = %(data_dir)s/dev_pro
raw_test_file = %(data_dir)s/test_raw
pro_test_file = %(data_dir)s/test_pro

min_occur_count = 2
dev_disambiguation_file = 
test_disambiguation_file = ../predicate-disambiguation/nugues/nugues-chinese-test-94.92.txt
dev_disambiguation_accuracy = 0.9492
test_disambiguation_accuracy = 0.9492

use_lm = True
lm_path = ../processed/chinese/sentences-chinese.txt.bert.chinese.cased.hdf5
lm_dims = 768
lm_hidden_size = 768
lm_sentences = ../processed/chinese-unified/sentences.txt
unified = True

[Save]
save_dir = ../logs/fasttext-bert-chinese-unified
config_file = %(save_dir)s/config.cfg
save_model_path = %(save_dir)s/model
save_vocab_path = %(save_dir)s/vocab
load_dir = ../logs/fast-elmo
load_model_path = %(load_dir)s/model
load_vocab_path = %(load_dir)s/vocab

[Network]

encoder_type = rnn

use_si_dropout = True

lstm_layers = 3
word_dims = 100
pretrain_dims = 300
lemma_dims = 100
head_flag_dims = 16
tag_dims = 100
dropout_emb = 0.2
lstm_hiddens = 400
dropout_lstm_input = 0.2
dropout_lstm_hidden = 0.2
mlp_rel_size = 300
dropout_mlp = 0.2


transformer_layers = 8
transformer_heads = 8
transformer_hiddens = 1024
transformer_ffn = 2048
transformer_dropout = 0.1
transformer_maxlen = 500
transformer_max_timescale = 1e4


[Optimizer]
optim = adam
lr_scheduler_type = default
warmup_steps = 2000
eta = 0.001
patience = 20
clip = 1.0
learning_rate = 2e-3
decay = .75
decay_steps = 5000
beta_1 = .9
beta_2 = .9
epsilon = 1e-12


[Run]
num_buckets_train = 30
num_buckets_valid = 10
num_buckets_test = 10
train_iters = 500000
train_batch_size = 5000
test_batch_size = 5000
validate_every = 1200
save_after = 5000

